---
title: "Section 4: Cross-validation and k-Nearest Neighbors"
format: html
editor: visual
---

# Section : **Cross-validation and k-Nearest Neighbors Overview**

In the **Cross-validation and k-Nearest Neighbors** section, you will learn about different types of discriminative and generative approaches for machine learning algorithms.

After completing this section, you will be able to:

-   Use the **k-nearest neighbors (kNN)** algorithm.

-   Understand the problems of **overtraining** and **oversmoothing**.

-   Use **cross-validation** to reduce the **true error** and the **apparent error**. 

This section has two parts: **nearest neighbors** and **cross-validation**. There are comprehension checks periodically throughout.

We encourage you to use R to interactively test out your answers and further your own learning. If you get stuck, we encourage you to search the discussion boards for the answer to your issue or ask us for help!

## 4.1: Nearest Neighbors

### **k-Nearest Neighbors (kNN)**

**Key point**

-   **K-nearest neighbors (kNN)** estimates the conditional probabilities in a similar way to bin smoothing. However, kNN is easier to adapt to multiple dimensions.

-   Using kNN, for any point (x1, x2) for which we want an estimate of p(x1, x2) , we look for the ***k*** **nearest points** to (x1, x2) and take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the **neighborhood**. Larger values of k result in smoother estimates, while smaller values of k result in more flexible and more wiggly estimates. 

-   To implement the algorithm, we can use the `knn3()` function from the **caret** package. There are two ways to call this function:

    1.  We can specify a formula and a data frame. The formula looks like y \~ x1 + x2 or y \~ . . The `predict()` function for `knn3` produces a probability for each class.

    2.  We can also call the function with the first argument being the matrix predictors and the second a vector of outcomes, like this:

```{r}
        x <- as.matrix(mnist_27$train[, -1])
        y <- mnist_27$train$y
        #knn_fit <- knn3(x,y)
```

-   Overtraining is when a machine learning model can make good predictions on the training set but cannot generalize well to new data. Overtraining is a reason why we have higher accuracy in the training set compared to the test set.

**Code**

```{r}
library(tidyverse)
library(caret)
library(dslabs)
library(gridExtra)


data("mnist_27")

mnist_27$test %>%
  ggplot(aes(x_1, x_2, color = y)) +
  geom_point()

knn_fit <- knn3(y ~ ., data = mnist_27$train)

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]

fit_lm <- mnist_27$train %>% 
  mutate(y = ifelse(y == 7, 1, 0)) %>% 
  lm(y ~ x_1 + x_2, data = .)
p_hat_lm <- predict(fit_lm, mnist_27$test)
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 7, 2))
confusionMatrix(y_hat_lm, mnist_27$test$y)$overall["Accuracy"]

plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster(show.legend = FALSE) +
    scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black")
}
p1 <- plot_cond_prob() +
  ggtitle("True conditional probability")
p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
  ggtitle("kNN-5 estimate")
grid.arrange(p2, p1, nrow=1)

y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall["Accuracy"]

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```

### **Overtraining and Oversmoothing**

**Key point**

-   With k-nearest neighbors (kNN), overtraining is at its worst when we set k = 1. With , the estimate for each (x1, x2) in the training set is obtained with just the y corresponding to that point.

-   Oversmoothing can occur when when k is too large and does not permit enough flexibility.

-   The goal of cross validation is to estimate quantities such as accuracy or expected MSE so we can pick the best set of tuning parameters for any given algorithm, such as the k for kNN.

**Code**

```{r}
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall["Accuracy"]

y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall["Accuracy"]

p1 <- mnist_27$true_p %>% 
  mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
  ggplot() +
  geom_point(data = mnist_27$train, aes(x_1, x_2, color= y), pch=21) +
  scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
  ggtitle("Train set")
p2 <- mnist_27$true_p %>% 
  mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
  ggplot() +
  geom_point(data = mnist_27$test, aes(x_1, x_2, color= y), 
             pch=21, show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
  ggtitle("Test set")
grid.arrange(p1, p2, nrow=1)

knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall["Accuracy"]

fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family="binomial")
p1 <- plot_cond_prob(predict(fit_glm, mnist_27$true_p)) +
  ggtitle("Regression")
p2 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) +
  ggtitle("kNN-401")
grid.arrange(p1, p2, nrow=1)
```

### **Comprehension Check: Nearest Neighbors**

NA

## 4.2: **Cross-validation**

### **Choosing k**

**Key point**

-   Due to overtraining, the accuracy estimates obtained with the test set will be generally lower than the estimates obtained with the training set.

-   We prefer to minimize the expected loss rather than the loss we observe in just one dataset. Also, if we were to use the test set to pick k, we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here we broke a golden rule of machine learning: we selected the  using the test set. Cross validation provides an estimate that takes these into account.

**Code**

```{r}
ks <- seq(3, 251, 2)

library(purrr)
accuracy <- map_df(ks, function(k){
  fit <- knn3(y ~ ., data = mnist_27$train, k = k)
  
  y_hat <- predict(fit, mnist_27$train, type = "class")
  cm_train <- confusionMatrix(y_hat, mnist_27$train$y)
  train_error <- cm_train$overall["Accuracy"]
  
  y_hat <- predict(fit, mnist_27$test, type = "class")
  cm_test <- confusionMatrix(y_hat, mnist_27$test$y)
  test_error <- cm_test$overall["Accuracy"]
  
  tibble(train = train_error, test = test_error)
})

accuracy %>% mutate(k = ks) %>%
  gather(set, accuracy, -k) %>%
  mutate(set = factor(set, levels = c("train", "test"))) %>%
  ggplot(aes(k, accuracy, color = set)) + 
  geom_line() +
  geom_point()

ks[which.max(accuracy$test)]
max(accuracy$test)
```

### **Mathematical description of cross-validation**

**Key point**

-   When we have just one dataset, we can estimate the MSE using the observed MSE. The theoretical MSE is often referred to as the true error, and the observed MSE as the apparent error.

-   There are two important characteristics of the apparent error to keep in mind. First, the apparent error is a random variable. Second, if we train an algorithm on the same dataset we use the compute the apparent error, we might be overtraining. In these cases, the apparent error will be an underestimate of the true error. Cross-validation helps to alleviate both of these problems.

-   Cross-validation helps imitate the theoretical set up of the true error as best as possible using the data we have. To do this, cross-validation randomly generates smaller datasets that are used to estimate the true error.

### **k-fold cross-validation**

**Key point**

-   To imitate an independent dataset, we divide the dataset into a training set and a test set. The training set is used for training our algorithms and the test set is used exclusively for evaluation purposes. Typically 10-20% of the data is set aside for testing.

-   We need to optimize algorithm parameters without using the test set, but if we optimize and evaluate on the same dataset we will overtrain. This is where cross-validation is useful.

-   To calculate MSE, we want to create several datasets that can be thought of as independent random samples. With k-fold cross-validation, we randomly split the data into k non-overlapping sets. We obtain k estimates of the MSE and then compute the average as a final estimate of our loss. Finally, we can pick the parameters that minimize this estimate of the MSE.

-   For a final evaluation of our algorithm, we often just use the test set.

-   In terms of picking k for cross-validation, larger values of are preferable but they will also take much more computational time. For this reason, the choices of 5 and 10 are common.

-   One way to improve the variance of our final estimate is to take more samples. We can do this by no longer requiring non-overlapping sets. The bootstrap can be thought of as a variation at which each fold observations are picked at random with replacement. This is the default approach of `caret::train`.

### **Bootstrap**

Bootstrapping allows us to approximate a Monte Carlo simulation without access to the entire distribution. We act as if the observed sample is the population. Next, we sample datasets (with replacement) of the same sample size as the original dataset. Finally, we compute the summary statistic, in this case the median, on these bootstrap samples.

Suppose the income distribution of your population is as follows:

```{r}
set.seed(1995)
n <- 10^6
income <- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I("black"))
```

The population median is given by the following code:

```{r}
m <- median(income)
```

The population median is 44939.

However, if we don't have access to the entire population but want to estimate the median, we can take a sample of 100 and estimate the population median m using the sample median M, like this:

```{r}
N <- 100
X <- sample(income, N)
median(X)
```

The sample median here is 38461.

Now let's consider constructing a confidence interval and determining the distribution of .

Because we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of using the following code:

```{r}
library(gridExtra)
B <- 10^4
M <- replicate(B, {
  X <- sample(income, N)
  median(X)
})
p1 <- qplot(M, bins = 30, color = I("black"))
p2 <- qplot(sample = scale(M), xlab = "theoretical", ylab = "sample") + 
  geom_abline()
grid.arrange(p1, p2, ncol = 2)
```

Knowing the distribution allows us to construct a confidence interval. However, as we have discussed before, in practice, we do not have access to the distribution. In the past, we have used the Central Limit Theorem (CLT), but the CLT we studied applies to averages and here we are interested in the median. If we construct the 95% confidence interval based on the CLT using the code below, we see that it is quite different from the confidence interval we would generate if we knew the actual distribution of M.

```{r}
median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)
```

The 95% confidence interval based on the CLT is (21017, 55904).

```{r}
quantile(M, c(0.025, 0.975))
```

The confidence interval based on the actual distribution of M is (34438, 59050).

The bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample (with replacement) datasets, of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on these bootstrap samples.

Theory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. We can construct bootstrap samples and an approximate distribution using the following code:

```{r}
B <- 10^4
M_star <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  median(X_star)
})
```

The confidence interval constructed using the bootstrap is much closer to the one constructed with the theoretical distribution, as you can see by using this code:

```{r}
quantile(M_star, c(0.025, 0.975))
```

The confidence interval from the bootstrap is (30253, 56909).

To learn more about the boostrap, including corrections one can apply to improve these confidence intervals, the book *An introduction to the bootstrap* by Efron and Tibshirani is a great resource.

Note that we can use ideas similar to those used in the bootstrap in cross-validation: instead of dividing the data into equal partitions, we can simply bootstrap many times.

**Key point**

-   When we don't have access to the entire population, we can use **bootstrap** to estimate the population median .

-   The bootstrap permits us to **approximate a Monte Carlo simulation** without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample datasets (with replacement) of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on this bootstrap sample.

-   Note that we can use ideas similar to those used in the bootstrap in **cross validation**: instead of dividing the data into equal partitions, we simply bootstrap many times.

**Code**

```{r}
# define the population distribution of income
set.seed(1995)
n <- 10^6
income <- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I("black"))

# calculate the population median
m <- median(income)
m

# estimate the population median
N <- 100
X <- sample(income, N)
M<- median(X)
M

# use a Monte Carlo simulation to learn the distribution of M
library(gridExtra)
B <- 10^4
M <- replicate(B, {
    X <- sample(income, N)
    median(X)
})
p1 <- qplot(M, bins = 30, color = I("black"))
p2 <- qplot(sample = scale(M), xlab = "theoretical", ylab = "sample") + geom_abline()
grid.arrange(p1, p2, ncol = 2)

# compare the 95% CI based on the CLT to the actual one
median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)
quantile(M, c(0.025, 0.975))

# bootstrap and approximate the distribution
B <- 10^4
M_star <- replicate(B, {
    X_star <- sample(X, N, replace = TRUE)
    median(X_star)
})

# look at the confidence interval from the bootstrap
quantile(M_star, c(0.025, 0.975))
```

### **Comprehension Check: Bootstrap**

NA
