---
title: "Section 6 Model Fitting and Recommendation Systems"
format: html
editor: visual
---

# Section: **Model Fitting and Recommendation Systems**

## **Overview**

In the **Model Fitting and Recommendation Systems** section, you will learn how to apply the machine learning algorithms you have learned.

After completing this section, you will be able to:

-   Apply the methods we have learned to an example, the **MNIST digits**.

-   Build a **movie recommendation system** using machine learning.

-   Penalize large estimates that come from small sample sizes using **regularization**.

This section has three parts: **case study: MNIST**, **recommendation systems**, and **regularization**. There are comprehension checks throughout.

We encourage you to use R to interactively test out your answers and further your own learning. If you get stuck, we encourage you to search the discussion boards for the answer to your issue or ask us for help!

## 6.1: **Case Study: MNIST**

### **MNIST case study: preprocessing**

**Key point**

-   We will apply what we have learned in the course on the Modified National Institute of Standards and Technology database (MNIST) digits, a popular dataset used in machine learning competitions.

    -   Common **preprocessing steps** include:

    1.  standardizing or transforming predictors and

    2.  removing predictors that are not useful, are highly correlated with others, have very few non-unique values, or have close to zero variation.

**Code**

```{r}
library(dslabs)
library(tidyverse)
mnist <- read_mnist()

names(mnist)
dim(mnist$train$images)

class(mnist$train$labels)
table(mnist$train$labels)

# sample 10k rows from training set, 1k rows from test set
set.seed(1990)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])

index <- sample(nrow(mnist$test$images), 1000)
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])

library(matrixStats)
sds <- colSds(x)
qplot(sds, bins = 256, color = I("black"))

library(caret)
nzv <- nearZeroVar(x)
image(matrix(1:784 %in% nzv, 28, 28))

col_index <- setdiff(1:ncol(x), nzv)
length(col_index)
```

### **MNIST case study: kNN**

**Key point**

-   The **caret** package requires that we **add column names** to the feature matrices.
-   In general, it is a good idea to **test out a small subset of the data** first to get an idea of how long your code will take to run.

**Code**

Note that your results may vary from those seen in the video as the seed was not set here.

```{r}
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)

control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(x[,col_index], y,
                                method = "knn", 
                                tuneGrid = data.frame(k = c(3,5,7)),
                                trControl = control)

n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index ,col_index], y[index],
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
fit_knn <- knn3(x[ ,col_index], y,  k = 3)

y_hat_knn <- predict(fit_knn,
                     x_test[, col_index],
                     type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]

cm$byClass[,1:2]
```

### **MNIST case study: random forest**

**Key point**

-   The **caret** package requires that we **add column names** to the feature matrices.

-   In general, it is a good idea to **test out a small subset of the data** first to get an idea of how long your code will take to run.

**Code**

```{r}
library(randomForest)
control <- trainControl(method="cv", number = 5)
grid <- data.frame(mtry = c(1, 5, 10, 25, 50, 100))
 

fit_rf <- randomForest(x[, col_index], y,
                  minNode = train_rf$bestTune$mtry)

y_hat_rf <- predict(fit_rf, x_test[ ,col_index])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]

```



### **MNIST case study: Variable Importance**

**Key points**

-   The **randomForest** package supports variable importance calculations.

-   An important part of data science is visualizing results to determine why we are failing.

**Note**

At 1:40 of the video, the errors for random forest obtained are different from the ones displayed in the video. The errors obtained are are actually the digits 4 (Pr(9) = 0.64), 3 (Pr(5) = 0.61), 7 (Pr(2) = 0.59) and 9 (Pr(0) = 0.59).

**Code**

```{r}
imp <- importance(fit_rf)
imp

mat <- rep(0, ncol(x))
mat[col_index] <- imp
image(matrix(mat, 28, 28))


p_max <- predict(fit_rf, x_test[,col_index], type = "prob") 
p_max <- p_max / rowSums(p_max)
p_max <- apply(p_max, 1, max)
  
ind  <- which(y_hat_rf != y_test)
ind <- ind[order(p_max[ind], decreasing = TRUE)]
  
rafalib::mypar(1,4)
for(i in ind[1:4]){
    image(matrix(x_test[i,], 28, 28)[, 28:1], 
                 main = paste0("Pr(",y_hat_rf[i],")=",round(p_max[i], 2),
                 " but is a ",y_test[i]),
                 xaxt="n", yaxt="n")
}

```

### **Ensembles**

**Key points**

-   **Ensembles** combine multiple machine learning algorithms into one model to improve predictions.

**Code**

```{r}
p_rf <- predict(fit_rf, x_test[,col_index], type = "prob")
p_rf <- p_rf / rowSums(p_rf)
p_knn <- predict(fit_knn, x_test[,col_index])
p <- (p_rf + p_knn)/2
y_pred <- factor(apply(p, 1, which.max)-1)
confusionMatrix(y_pred, y_test)$overall["Accuracy"]
```

### **Comprehension Check: Ensembles**

NA

## 6.2: **Recommendation Systems**

### **Recommendation Systems**

**Netflix Challenge links**

For more information about the "Netflix Challenge," you can check out these sites:

-   [https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/ External link](https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/)

-   [http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/ External link](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)

-   [https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf External link](https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf)

**Key points**

-   **Recommendation systems** are more complicated machine learning challenges because each outcome has a different set of predictors. For example, different users rate a different number of movies and rate different movies.
-   To compare different models or to see how well we’re doing compared to a baseline, we will use **root mean squared error (RMSE) as our loss function**. We can interpret RMSE similar to standard deviation.
-   If N is the number of user-movie combinations, yu,iis the rating for movie i by user u, and \^yu,i is our prediction, then **RMSE is defined as follows**: 

$$\sqrt{ \frac{1}{N} \sum_{u, i} ( \hat{y}_{u, i} - y_{u, i} )^2}$$

**Code**

 
```{r}
# HarvardX: PH125.8x
# Data Science: Machine Learning
# R code from course videos

# Model Fitting and Recommendation Systems

## Recommendation Systems

### Recommendation Systems

# https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/
# http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/
# https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf

library(dslabs)
library(tidyverse)

data("movielens")

movielens %>% as_tibble()

movielens %>% 
     summarize(n_users = n_distinct(userId),
               n_movies = n_distinct(movieId))

keep <- movielens %>%
     dplyr::count(movieId) %>%
     top_n(5) %>%
     pull(movieId)

tab <- movielens %>%
     filter(userId %in% c(13:20)) %>% 
     filter(movieId %in% keep) %>% 
     dplyr::select(userId, title, rating) %>% 
     pivot_wider(names_from="title", values_from="rating")
tab %>% knitr::kable()

users <- sample(unique(movielens$userId), 100)
rafalib::mypar()
movielens %>% filter(userId %in% users) %>% 
dplyr::select(userId, movieId, rating) %>%
mutate(rating = 1) %>%
pivot_wider(names_from = movieId, values_from = rating) %>% 
(\(mat) mat[, sample(ncol(mat), 100)])()%>%
as.matrix() %>% 
t() %>%
image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")

library(gridExtra)
p1 <- movielens %>% 
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies")

p2 <- movielens %>% 
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Users")
grid.arrange(p1, p2, ncol = 2)

library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

#To make sure we don't include users and movies in the test set that do not appear in the training set, we removed these using the semi_join function, using this simple code.
test_set <- test_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")

RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}

### Building the Recommendation System
# A first model
mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse

predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

# Modeling movie effects

# fit <- lm(rating ~ as.factor(userId), data = movielens)
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))


movie_avgs %>% qplot(b_i, geom ="histogram", bins = 30, data = ., color = I("black"))

predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()

# Modeling user effects

train_set %>% 
     group_by(userId) %>% 
     summarize(b_u = mean(rating)) %>% 
     filter(n()>=100) %>%
     ggplot(aes(b_u)) + 
     geom_histogram(bins = 30, color = "black")

# lm(rating ~ as.factor(movieId) + as.factor(userId))
user_avgs <- train_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     pull(pred)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

**Notes on Recommendation Systems**

-   **Recommendation systems** use ratings that the users give an item on buying and/or using them to make specific recommendations.Companies like Amazon collect massive datasets of user ratings on the products sold to them. The datasets are subsequently used to predict a high rated items for a given user and are then recommended to the user.

-   Similarly, Netflix use movie ratings provided by users to predict a high rated movie for a given user and then recommended it to the user. Usually the movie ratings are on a 1-5 scale, where 5 represents an excellent movie and 1 suggests it to be a poor one.

-   Here we describe the basics of how the movie recommendations are made, motivated by some of the approaches taken by the winners of the *Netflix Challenges*.

-   In October 2006, Netflix offered a challenge on improving their movie recommendation system by 10% to win a prize money of a million dollars. The details of the challenge and the summary of the winning algorithm can be found in the *Netflix Challenge links*. Here we describe some of the data analysis strategies used by the winners of the challenge.

-   [**Movielens data**]{.underline}

-   Though the Netflix data is not available publicly, a database with over 20 million ratings for over 27,000 movies rated by more than 138,000 was generated by the GroupLens research lab. We made a small subset of this data and the data is available in **dslabs** package.

-   **The data can be uploaded** as follows:

    ```{r}
    library(dslabs)
    library(tidyverse)
    data("movielens")
    ```

It can be viewed in a tidy format with thousands of rows.

Each row in the table represents a rating given by one user to one movie.

-   The **number of unique users** who provided ratings and the **number unique movies** that were rated can be viewed as follows:

```{r}
movielens %>%
  summarize(n_users = n_distinct(userId),
  n_movies = n_distinct(movieId))
```

Multiplying the number of unique users (n_users = 671) and the number unique movies (n_movies = 9066) gives a total more than 5 million. However, our movielens data table has 100004 rows. This implies that not every movie is rated by every user. Therefore it is obvious that if we generate a table with the unique users as rows and the movies as columns, there will be a lot many empty cells. This is evident in a small example table consisting of seven users and five movies.

**Filling in the NAs in the above table can be thought as a task of recommendation system.**The following matrix consisting of a random sample of 100 movies and 100 users show how sparse the matrix is. The yellow dots represent the user/movie combination for which a rating is available.

In this machine learning challenge, suppose we are predicting the rating for **movie i** by **user** u. In that case, all ratings related to movie i and by user u may be used as predictors (not to forget that different users rate different movies and different number of movies). Also, ratings of movies similar to movie i and by users similar to user u may also be used as predictors. Thus for prdiction of each outcome Y in this case, a different set of predictors can be used.

**Some general characteristics of the data:** It is evident from the distributions plotted below that some movies are rated more than the other movies and some users are more active and rate more number of movies than the other users do.

As a part of machine learning challenge, **we need to build an algorithm with this data that we have collected**. Others should be able to use the algorithm to suggest movies to users when they look for movie recommendations.

We **create train set and test set** that is required to assess the accuracy of the models we implement. Here we use **`semi_join()`** function to ensure that we don’t include users and movies in the test set that do not appear in the training set. The code used for the purpose is as follows:

```{r}
library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

test_set <- test_set %>% 
 semi_join(train_set, by = "movieId") %>%
 semi_join(train_set, by = "userId")
```

**Loss function:**To compare different models or to see how well we're doing compared to some baseline, the typical error loss, the **residual mean squared error (RMSE)** is used on the test set. We can interpret RMSE similar to standard deviation.

If is the number of user-movie combinations,  is the rating for movie by user , and  is our prediction, then **RMSE is defined as follows**: 

$$\sqrt{ \frac{1}{N} \sum_{u, i} ( \hat{y}_{u, i} - y_{u, i} )^2}$$ - In the next step, we build models and compare them to each other. It is described in the next reading section.

### **Building the Recommendation System**

**Key points**

-   We start with a model that **assumes the same rating for all movies and all users**, with all the differences explained by random variation: If µ represents the true rating for all movies and users and represents independent errors sampled from the same distribution centered at zero, then:  $$Y_{u, i} = \mu + \epsilon_{u, i}$$

-   In this case, the **least squares estimate of** µ - the estimate that minimizes the root mean squared error — is the average rating of all movies across all users.

-   We can improve our model by adding a term, bi , that represents the **average rating for movie** i: $$Y_{u, i} = \mu + b_i + \epsilon_{u, i}$$

-   bi is the average of Yu,i minus the overall mean for each movie i.

-   We can further improve our model by adding , the **user-specific effect**: $$Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i}$$

-   Note that because there are thousands of 's, the `lm()` function will be very slow or cause R to crash, so **we don’t recommend using linear regression to calculate these effects**.

**Code**

```{r}
# HarvardX: PH125.8x
# Data Science: Machine Learning
# R code from course videos

# Model Fitting and Recommendation Systems

## Recommendation Systems

### Recommendation Systems

# https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/
# http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/
# https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf

library(dslabs)
library(tidyverse)

data("movielens")

movielens %>% as_tibble()

movielens %>% 
     summarize(n_users = n_distinct(userId),
               n_movies = n_distinct(movieId))

keep <- movielens %>%
     dplyr::count(movieId) %>%
     top_n(5) %>%
     pull(movieId)

tab <- movielens %>%
     filter(userId %in% c(13:20)) %>% 
     filter(movieId %in% keep) %>% 
     dplyr::select(userId, title, rating) %>% 
     pivot_wider(names_from="title", values_from="rating")
tab %>% knitr::kable()

users <- sample(unique(movielens$userId), 100)
rafalib::mypar()
movielens %>% filter(userId %in% users) %>% 
dplyr::select(userId, movieId, rating) %>%
mutate(rating = 1) %>%
pivot_wider(names_from = movieId, values_from = rating) %>% 
(\(mat) mat[, sample(ncol(mat), 100)])()%>%
as.matrix() %>% 
t() %>%
image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")

library(gridExtra)
p1 <- movielens %>% 
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies")

p2 <- movielens %>% 
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Users")
grid.arrange(p1, p2, ncol = 2)

library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

#To make sure we don't include users and movies in the test set that do not appear in the training set, we removed these using the semi_join function, using this simple code.
test_set <- test_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")

RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}

### Building the Recommendation System
# A first model
mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse

predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

# Modeling movie effects

# fit <- lm(rating ~ as.factor(userId), data = movielens)
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))


movie_avgs %>% qplot(b_i, geom ="histogram", bins = 30, data = ., color = I("black"))

predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()

# Modeling user effects

train_set %>% 
     group_by(userId) %>% 
     summarize(b_u = mean(rating)) %>% 
     filter(n()>=100) %>%
     ggplot(aes(b_u)) + 
     geom_histogram(bins = 30, color = "black")

# lm(rating ~ as.factor(movieId) + as.factor(userId))
user_avgs <- train_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     pull(pred)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

### Notes on building the recommendation system

**A first model**

-   **Building the first model- the simplest possible recommendation system:** This model assumes the same rating for all movies and users with all the differences explained by random variation. The model is as follows: $$Y_{u, i} = \mu + \epsilon_{u, i}$$

-   where, represents the true rating for all movies and users and represents independent errors sampled from the same distribution centered at zero.

-   We know that the estimate that minimizes the RMSE is the least squares estimate of µ and, in this case, is the average of all ratings. Suppose \^µ represents the average of all ratings and we use \^µ to predict all the unknown movie ratings. **From our data table, we obtain** \^µ **and RMSE as shown below:**

```{r}
RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}

mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse
```

-   The following code confirms that any number other than would result into a higher RMSE.

```{r}
 
predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)
```

**Modeling movie effects** - We know that some movies are just generally rated higher than others. Our first model can be improved by adding a term *bi* that represents the average ranking for movie *i*. The improved model can be described as follows: $$Y_{u, i} = \mu + b_i + \epsilon_{u, i}$$

-   We can again use **least squares** to estimate the *b~i~* in the following way:

```{r}
fit <- lm(rating ~ as.factor(userId), data = movielens)
```

However, the lm() function would be very slow here beacause there are thousands of $b_i$s as each movie gets a $b_i$. Therefore, **we don't recommend running the code above**. But in this particular situation, we know that the least squares estimate $\hat b_i$ is just the average of $Y_{u,i} - \hat\mu$ for each movie $i$. So we can compute them as shown below (Note that to make the code look cleaner, we are not using the $hat$ notation from this point onwards):

```{r}
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))
```

-   We can see in the plot below that, **these estimates** $\hat b_i$ for movies vary substantially. This is not surprising as we know that some movies are good whereas others are not.

-   We had estimated the $\hat\mu$, the average of all ratings, as 3.5. So a $\hat b_i$ of 1.5 implies $\hat y_{u,i} = \hat\mu + \hat b_i = 3.5 + 1.5 = 5$ which is a perfect five-star rating.

-   Now let's see how much our prediction improves once we predict using the model that we just fit. **We can use this code and see that our residual mean squared error did drop a little bit**.

```{r}
predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
model_1_rmse
```

**Modeling user effects**

-   We use the following code to compute the **average rating for user µ for those that have rated 100 or more movies** and to plot the same.

```{r}
train_set %>% 
     group_by(userId) %>% 
     summarize(b_u = mean(rating)) %>% 
     filter(n()>=100) %>%
     ggplot(aes(b_u)) + 
     geom_histogram(bins = 30, color = "black")
```

-   The **substantial variability across users is quiet evident from the above plot**. This implies that a further improvement to our model may be: $$Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i}$$

where, $b_u$ is the user-specific effect. Now if a cranky user (negative $b_u$) rates a great movie (positive $b_i$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.

-   Though the above model can be fit with the following line of code, **we don't recommend running the code for reasons explained earlier**.

```{r}
lm(rating ~ as.factor(movieId) + as.factor(userId))

```

-   Instead, we will compute an approximation by computing $\hat\mu$ and $\hat b_i$ and estimating $\hat b_u$ as the average of $\hat y_{u,i} - \hat\mu - \hat b_i$:

```{r}
user_avgs <- train_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))
```

-   We can now construct predictors and see how much the RMSE improves using the code below:

```{r}
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     pull(pred)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
model_2_rmse
```

**Comparison of the models** - A comparison of all the three models that are built till now are tabulated below:

![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@Comparison_table.png)

### **Comprehension Check: Recommendation Systems**

NA

```{r}
data("movielens")

movielens %>% group_by(movieId) %>%
	summarize(n = n(), year = as.character(first(year))) %>%
	qplot(year, n, data = ., geom = "boxplot") +
	coord_trans(y = "sqrt") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

movielens %>% 
	filter(year >= 1993) %>%
	group_by(movieId) %>%
	summarize(n = n(), years = 2018 - first(year),
				title = title[1],
				rating = mean(rating)) %>%
	mutate(rate = n/years) %>%
	top_n(25, rate) %>%
	arrange(desc(rate))


movielens %>% 
    filter(year >= 1993) %>%
    group_by(movieId) %>%
    summarize(n = n(), years = 2018 - first(year),
                title = title[1],
                rating = mean(rating)) %>%
    mutate(rate = n/years) %>%
    ggplot(aes(rate, rating)) +
    geom_point() +
    geom_smooth()

```


## 6.3: **Regularization**

### **Regularization**

**Code**

```{r}
# HarvardX: PH125.8x
# Data Science: Machine Learning
# R code from course videos

# Model Fitting and Recommendation Systems

## Regularization

### Regularization

library(dslabs)
library(tidyverse)
library(caret)
data("movielens")
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]
test_set <- test_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")
RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}
mu_hat <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu_hat)
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))
predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
user_avgs <- train_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     pull(pred)
model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))

test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     mutate(residual = rating - (mu + b_i)) %>%
     arrange(desc(abs(residual))) %>% 
     dplyr::select(title,  residual) %>% slice(1:10) %>% pull(title)

movie_titles <- movielens %>% 
     dplyr::select(movieId, title) %>%
     distinct()

# 10 best
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     pull(title)

# 10 worst
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     pull(title)

train_set %>% count(movieId) %>% 
     left_join(movie_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     slice(1:10) %>% 
     pull(n)

train_set %>% dplyr::count(movieId) %>% 
     left_join(movie_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     slice(1:10) %>% 
     pull(n)

lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 


tibble(original = movie_avgs$b_i, 
           regularlized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
     ggplot(aes(original, regularlized, size=sqrt(n))) + 
     geom_point(shape=1, alpha=0.5)

train_set %>%
     count(movieId) %>% 
     left_join(movie_reg_avgs, by="movieId") %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     pull(title)

train_set %>%
     dplyr::count(movieId) %>% 
     left_join(movie_reg_avgs, by="movieId") %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     pull(title)

predicted_ratings <- test_set %>% 
     left_join(movie_reg_avgs, by='movieId') %>%
     mutate(pred = mu + b_i) %>%
     pull(pred)

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()

lambdas <- seq(0, 10, 0.25)
mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
     group_by(movieId) %>% 
     summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
     predicted_ratings <- test_set %>% 
          left_join(just_the_sum, by='movieId') %>% 
          mutate(b_i = s/(n_i+l)) %>%
          mutate(pred = mu + b_i) %>%
          pull(pred)
     return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
     mu <- mean(train_set$rating)
     b_i <- train_set %>% 
          group_by(movieId) %>%
          summarize(b_i = sum(rating - mu)/(n()+l))
     b_u <- train_set %>% 
          left_join(b_i, by="movieId") %>%
          group_by(userId) %>%
          summarize(b_u = sum(rating - b_i - mu)/(n()+l))
     predicted_ratings <- 
          test_set %>% 
          left_join(b_i, by = "movieId") %>%
          left_join(b_u, by = "userId") %>%
          mutate(pred = mu + b_i + b_u) %>%
          pull(pred)
     return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

#### **Notes on regularization**

**Motivation behind using regularization**

-   To improve our results, we will use **regularization**.

-   On including the movie effects, we observed that the RMSE reduced from 1.048 to 0.986. Despite the large movie to movie variation, our improvement in RMSE was only about . To understand the problem with the model that included only movie effects, let's look at **the 10 largest mistakes predicted by the model**:

    ```{r}
    library(dslabs)
    test_set %>% 
         left_join(movie_avgs, by='movieId') %>%
         mutate(residual = rating - (mu + b_i)) %>%
         arrange(desc(abs(residual))) %>% 
         select(title,  residual) %>% slice(1:10) %>% pull(title)
    ```

-   They seem like obscure movies. Almost all of them have very extreme predicted ratings. To make a better sense, let's look at the top 10 worst and best movies based on $\hat b_i$. The relevant codes and the list of the titles of the top 10 best and worst movies based on $\hat b_i$ are listed below:

```{r}
  
movie_titles <- movielens %>% 
     dplyr::select(movieId, title) %>%
     distinct()

movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     pull(title)

movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     pull(title)
```

The titles of the 10 best movies based on $\hat b_i$ are: The titles of the 10 worst movies based on $\hat b_i$ are:

-   They all seem to be obscure movies. It is important to look at how often these movies are rated in our data table. The code used for the purpose is as follows:

    ```{r}
    train_set %>% count(movieId) %>% 
         left_join(movie_avgs) %>%
         left_join(movie_titles, by="movieId") %>%
         arrange(desc(b_i)) %>% 
         slice(1:10) %>% 
         pull(n)

    train_set %>% dplyr::count(movieId) %>% 
         left_join(movie_avgs) %>%
         left_join(movie_titles, by="movieId") %>%
         arrange(b_i) %>%
         slice(1:10) %>% 
         pull(n)
    ```

-   The supposed “best” and “worst” movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones because with fewer users there's a lot of uncertainty in the estimation. Therefore, larger estimates of $\hat b_i$, negative or positive, are more likely.

-   Large errors can increase our RMSE, so we would rather be conservative when unsure.

-   **Regularization** permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions.

#### Penalized least squares

[Using regularization to estimate the **movie effect**]{.underline}

-   To estimate the $b$’s, we will now **minimize this equation**, which contains a penalty term: $$\frac{1}{N}\sum_{u, i}(y_{u, i}-\mu-b_i)^2 + \lambda\sum_i b_{i}^2$$

The first term is the **mean squared error** and the second is a **penalty term** that gets larger when many $b$’s are large.

The values of $b$ that minimize this equation are given by: $$\hat{b}_{i}(\lambda) = \frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}(Y_{u, i} - \hat{\mu}),$$

where $n_i$ is a number of ratings$b$ for movie $i$.

-   <div>

    -   The **larger** $\lambda$ is, the more we shrink. is a tuning parameter, so we can use cross-validation to choose it. We should be using full cross-validation on just the training set, without using the test set until the final assessment.

    -   Using $\lambda = 3$, let's see how the estimates shrink by looking at the plot of the regularized estimates versus the least squares estimates.

        ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@reg_vsori_est.tiff)

    -   Using the penalized estimates of $\hat b_i(\lambda)$, the list of the titles of the top 10 best and the top 10 worst movies now make sense. The list of best movies now consists of the movies that are watched more and have more ratings. The list of the top 10 best and top 10 worst movies based on penalized estimates of $\hat b_i(\lambda)$ are provided here.

    -   The list of the top 10 best movies based on $\hat b_i(\lambda)$:

        ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@10_best_reg.png)

        The list of the top 10 worst movies based on $\hat b_i(\lambda)$:

        ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@10_worst_reg.png)

        [Using regularization to estimate the **user effect**]{.underline}

    -   We will now minimize this equation: $$\frac{1}{N}\sum_{u, i}(y_{u, i}-\mu-b_i-b_u)^2 + \lambda(\sum_i b_{i}^2 + \sum_u b_{u}^2)$$

    -   The estimates that minimize the equation can be found similarly to what we did earlier for the regularized movie effect model equation. In this case also, to estimate , we should use full cross-validation on just the training set, without using the test set until the final assessment.

    -   

    </div>

**Comparison of the models**

-   A **comparison** of all the five models that are built till now are tabulated below. It shows that the penalized estimates provide a large improvement over the least squares estimates.

![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@Comparison_five.png)

### **Comprehension Check: Regularization**

NA


```{r}
data("movielens")
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]
test_set <- test_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")
RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}
mu_hat <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu_hat)
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))
predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     .$b_i
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
user_avgs <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     .$pred
model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))

test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     mutate(residual = rating - (mu + b_i)) %>%
     arrange(desc(abs(residual))) %>% 
     dplyr::select(title,  residual) %>% slice(1:10) %>% knitr::kable()

movie_titles <- movielens %>% 
     dplyr::select(movieId, title) %>%
     distinct()
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     knitr::kable()

movie_avgs %>% left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i) %>% 
     slice(1:10) %>%  
     knitr::kable()

train_set %>% dplyr::count(movieId) %>% 
     left_join(movie_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     knitr::kable()

train_set %>% dplyr::count(movieId) %>% 
     left_join(movie_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     knitr::kable()
lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

data_frame(original = movie_avgs$b_i, 
           regularlized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
     ggplot(aes(original, regularlized, size=sqrt(n))) + 
     geom_point(shape=1, alpha=0.5)

train_set %>%
     dplyr::count(movieId) %>% 
     left_join(movie_reg_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(desc(b_i)) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     knitr::kable()

train_set %>%
     dplyr::count(movieId) %>% 
     left_join(movie_reg_avgs) %>%
     left_join(movie_titles, by="movieId") %>%
     arrange(b_i) %>% 
     dplyr::select(title, b_i, n) %>% 
     slice(1:10) %>% 
     knitr::kable()

predicted_ratings <- test_set %>% 
     left_join(movie_reg_avgs, by='movieId') %>%
     mutate(pred = mu + b_i) %>%
     .$pred

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()

lambdas <- seq(0, 10, 0.25)
mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
     group_by(movieId) %>% 
     summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
     predicted_ratings <- test_set %>% 
          left_join(just_the_sum, by='movieId') %>% 
          mutate(b_i = s/(n_i+l)) %>%
          mutate(pred = mu + b_i) %>%
          .$pred
     return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
     mu <- mean(train_set$rating)
     b_i <- train_set %>%
          group_by(movieId) %>%
          summarize(b_i = sum(rating - mu)/(n()+l))
     b_u <- train_set %>% 
          left_join(b_i, by="movieId") %>%
          group_by(userId) %>%
          summarize(b_u = sum(rating - b_i - mu)/(n()+l))
     predicted_ratings <- 
          test_set %>% 
          left_join(b_i, by = "movieId") %>%
          left_join(b_u, by = "userId") %>%
          mutate(pred = mu + b_i + b_u) %>%
          .$pred
     return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()

```

```{r}
# set.seed(1986) # if using R 3.5 or earlier
set.seed(1986, sample.kind="Rounding") # if using R 3.6 or later
n <- round(2^rnorm(1000, 8, 1))

# set.seed(1) # if using R 3.5 or earlier
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
mu <- round(80 + 2*rt(1000, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:1000),
                      size = n,
                      quality = mu,
                      rank = rank(-mu))

schools %>% top_n(10, quality) %>% arrange(desc(quality))

# set.seed(1) # if using R 3.5 or earlier
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
mu <- round(80 + 2*rt(1000, 5))

scores <- sapply(1:nrow(schools), function(i){
       scores <- rnorm(schools$size[i], schools$quality[i], 30)
       scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))

schools %>% top_n(10, score) %>% arrange(desc(score)) %>% dplyr::select(id, size, score)

median(schools$size)
schools %>% top_n(10, score) %>% .$size %>% median()


median(schools$size)
schools %>% top_n(-10, score) %>% .$size %>% median()


overall <- mean(sapply(scores, mean))


alpha <- 25
score_reg <- sapply(scores, function(x)  overall + sum(x-overall)/(length(x)+alpha))
schools %>% mutate(score_reg = score_reg) %>%
	top_n(10, score_reg) %>% arrange(desc(score_reg))


alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
	score_reg <- sapply(scores, function(x) overall+sum(x-overall)/(length(x)+alpha))
	sqrt(mean((score_reg - schools$quality)^2))
})
plot(alphas, rmse)
alphas[which.min(rmse)]


alpha <- alphas[which.min(rmse)]  
score_reg <- sapply(scores, function(x)
	overall+sum(x-overall)/(length(x)+alpha))
schools %>% mutate(score_reg = score_reg) %>%
	top_n(10, score_reg) %>% arrange(desc(score_reg))

 
alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
	score_reg <- sapply(scores, function(x) sum(x)/(length(x)+alpha))
	sqrt(mean((score_reg - schools$quality)^2))
})
plot(alphas, rmse)
alphas[which.min(rmse)]
```



### **Matrix Factorization**

**Notes on matrix factorization**

[**Matrix factorization in the context of movie recommendation system**]{.underline}

-   Our earlier model,$Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i}$, accounts for movie to movie differences through the $b_i$ and user to user differences through the $b_u$. However, it fails to account for an important source of variation related to the fact that groups of movies and groups of users have similar rating patterns. We can observe these patterns by studying the residuals and **converting our data into a matrix where each user gets a row and each movie gets a column**: $r_{u, i} = y_{u, i} - \hat{b}_i - \hat{b}_u,$ where $y_{u, i}$ is the entry in row and column $i$.

-   Here for illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. The movie Scent of a Woman (movieId == 3252) is kept in the small subset because we use it for a specific example. The subset of movies is created using the following code:

    ```{r}
     train_small <- movielens %>%       group_by(movieId) %>%      filter(n() >= 50 | movieId == 3252) %>% ungroup() %>%      group_by(userId) %>%      filter(n() >= 50) %>% ungroup()  y <- train_small %>%       dplyr::select(userId, movieId, rating) %>%      pivot_wider(names_from = "movieId", values_from = "rating") %>%      as.matrix()  rownames(y)<- y[,1] y <- y[,-1]  movie_titles <- movielens %>%    dplyr::select(movieId, title) %>%   distinct()  colnames(y) <- with(movie_titles, title[match(colnames(y), movieId)])  
    ```

-   Our data subset can be converted to residuals by removing the column and row effects:

    ```{r}
     y <- sweep(y, 1, rowMeans(y, na.rm=TRUE)) y <- sweep(y, 2, colMeans(y, na.rm=TRUE))
    ```

-   If the model above describes all the signal and the $\epsilon_{u, i}$s are just noise, then the residuals for different movies should be independent of each other. However from the following plots and the table with pairwise correlation between a set of five movies, it is evident that there's a pattern in the data.

-   [The plots:]{.underline}

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@cor_plot.tiff)

    The first plot indicates that the users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. The next two plots say that the same is true between The Godfather and Goodfellas and between You’ve Got Mail and Sleepless in Seattle.

    [The pairwise correlation table:]{.underline}

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@pair_cor_table.png)

-   To account for the structure in the data, we use matrix factorization.

-   [**Factor analysis**]{.underline}

-   Here is an illustration, using a simulation, of how we can use some structure to predict the $r_{u, i}$. Suppose our $r$ looks as shown below:

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@res_img.png)

    In the above table, we see very strong correlation patterns:

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@cor_r.png)

-   We can create vectors *q* and *p*, that can explain much of the structure we see. The *q* and *p* would look as follows:

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@q_img.png)

    The vector *q* narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1).

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@img_p.png)

    The vector *p* narrows down the users into three groups: those that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct , which has 60 values, with a couple of vectors totaling 17 values.

-   We can **factorize the matrix of residuals** into a *q* vector and vector $q, r_{u, i} \approx p_u q_i$, allowing us to explain more of the variance using a model like this: $$Y_{u, i} = \mu + b_i + b_u + p_u q_i + \epsilon_{i, j}$$

-   We motivated the need for the $p_u q_i$ term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor $q_i$ that determined which of the two genres movie belongs to. But the structure in our movie $i$ data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation by adding a sixth movie. Now the residuals and the correlation in the new dataset look as follows:

    [The residuals in new dataset]{.underline}

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@r_newds.png)

    [The correlation structure in the new dataset]{.underline}

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@cor_newds.png)

-   The overall structure of the correlation obtained from the new simulated dataset is not that far off the real correlation:

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@real_cor.png)

-   To explain this more complicated structure, we need two factors. They are as explained below:

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@new_q.png)

    The first factor (the first row) narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). The second factor (the second row) is used to explain the Al Pacino versus no Al Pacino groups. We also need the following two sets of coefficients to describe the users.

    ![](https://courses.edx.org/asset-v1:HarvardX+PH125.8x+3T2022+type@asset+block@p_new.png)

-   Because our example is more complicated, we can use **two factors to explain the structure and two sets of coefficients to describe users**: $$Y_{u, i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \epsilon_{i, j}$$

    The model has 36 parameters that can be used to explain much of the variability in the 72 ratings

-   To estimate factors using our data instead of constructing them ourselves, we can use **principal component analysis (PCA) or singular value decomposition (SVD)**.This is discussed in the next video.

### **SVD and PCA**

**Key points**

-   You can think of **singular value decomposition (SVD)** as an algorithm that finds the vectors $p$ and $q$ that permit us to write the matrix of residuals $r$ with $m$ rows and $n$ columns in the following way: $$r_{u, i} = p_{u, 1} q_{1, i} + p_{u, 2} q_{2, i} + ... + p_{u, m} q_{m, i},$$

    with the variability of these terms decreasing and the $p$’s uncorrelated to each other.

-   **SVD also computes the variabilities** so that we can know how much of the matrix’s total variability is explained as we add new terms.

-   The **vectors** $q$ are called the principal components and the **vectors** $p$ are the user effects. By using principal components analysis (PCA), matrix factorization can capture structure in the data determined by user opinions about movies.

**Code**

```{r}
y[is.na(y)] <- 0
y <- sweep(y, 1, rowMeans(y))
pca <- prcomp(y)

dim(pca$rotation)

dim(pca$x)

plot(pca$sdev)

var_explained <- cumsum(pca$sdev^2/sum(pca$sdev^2))
plot(var_explained)

library(ggrepel)
pcs <- data.frame(pca$rotation, name = colnames(y))
pcs %>%  ggplot(aes(PC1, PC2)) + geom_point() + 
     geom_text_repel(aes(PC1, PC2, label=name),
                     data = filter(pcs, 
                                   PC1 < -0.1 | PC1 > 0.1 | PC2 < -0.075 | PC2 > 0.1))

pcs %>% select(name, PC1) %>% arrange(PC1) %>% slice(1:10)

pcs %>% select(name, PC1) %>% arrange(desc(PC1)) %>% slice(1:10)

pcs %>% select(name, PC2) %>% arrange(PC2) %>% slice(1:10)

pcs %>% select(name, PC2) %>% arrange(desc(PC2)) %>% slice(1:10)
```

### **Comprehension Check: Matrix Factorization**

NA

